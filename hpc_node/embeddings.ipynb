{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\harsh\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\langchain\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='6f000b14-c429-4cea-bb51-b3b9ef97567a', embedding=None, metadata={'page_label': '1', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Embeddings_Rag\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-08', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='How Good Are Low-bit Quantized LLAMA3 Models?\\nAn Empirical Study\\nWei Huang∗\\nThe University of Hong Kong\\nweih@connect.hku.hkXudong Ma∗\\nBeihang University\\nmacaronlin@buaa.edu.cn\\nHaotong Qin†\\nETH Zurich\\nhaotong.qin@pbl.ee.ethz.chXingyu Zheng\\nBeihang University\\nxingyuzheng@buaa.edu.cn\\nChengtao Lv\\nBeihang University\\nlvchengtao@buaa.edu.cnHong Chen\\nBeihang University\\n18373205@buaa.edu.cnJie Luo\\nBeihang University\\nluojie@buaa.edu.cn\\nXiaojuan Qi\\nThe University of Hong Kong\\nxjqi@eee.hku.hkXianglong Liu\\nBeihang University\\nxlliu@buaa.edu.cnMichele Magno\\nETH Zurich\\nmichele.magno@pbl.ee.ethz.ch\\nAbstract\\nMeta’s LLAMA family has become one of the most powerful open-source Large\\nLanguage Model (LLM) series. Notably, LLAMA3 models have recently been\\nreleased and achieve impressive performance across various with super-large scale\\npre-training on over 15T tokens of data. Given the wide application of low-\\nbit quantization for LLMs in resource-limited scenarios, we explore LLAMA3 ’s\\ncapabilities when quantized to low bit-width. This exploration holds the potential to\\nunveil new insights and challenges for low-bit quantization of LLAMA3 and other\\nforthcoming LLMs, especially in addressing performance degradation problems\\nthat suffer in LLM compression. Specifically, we evaluate the 10 existing post-\\ntraining quantization and LoRA-finetuning methods of LLAMA3 on 1-8 bits\\nand diverse datasets to comprehensively reveal LLAMA3 ’s low-bit quantization\\nperformance. Our experiment results indicate that LLAMA3 still suffers non-\\nnegligent degradation in these scenarios, especially in ultra-low bit-width. This\\nhighlights the significant performance gap under low bit-width that needs to be\\nbridged in future developments. We expect that this empirical study will prove\\nvaluable in advancing future models, pushing the LLMs to lower bit-width with\\nhigher accuracy for being practical. Our project is released on https://github.\\ncom/Macaronlin/LLaMA3-Quantization and quantized LLAMA3 models are\\nreleased in https://huggingface.co/LLMQ .\\n1 Introduction\\nLaunched by Meta in February 2023, the LLaMA [ 18] series2represents a breakthrough in autore-\\ngressive large language models (LLMs) using the Transformer [ 19] architecture. Right from its first\\n∗Equal Contribution.†Corresponding Author.\\n2https://llama.meta.comarXiv:2404.14047v1  [cs.LG]  22 Apr 2024', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='4c02a544-3425-486a-b3b5-707b3ee4c0e2', embedding=None, metadata={'page_label': '2', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Embeddings_Rag\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-08', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Evaluated LLMsLLaMA3-8BLLaMA3-70B1QuantizationMethodsRTN2GPTQAWQSmoothQuantPB-LLMBiLLMQuIPDB-LLMQLoRAIR-QLoRAEvaluationDatasetsWikiText2C4PTBPIQAARC-eARC-cHellaSwag3WinograndePerplexity↓CommonSenseQA↑Post-Training QuantizationLoRA-FinetuningHumanitiesSTEMSocialOtherMMLU↑Figure 1: The overview of our empirical study\\nversion, with 13 billion parameters, it managed to outperform the much larger, closed-source GPT-3\\nmodel which boasts 175 billion parameters. On April 18, 2024, Meta introduced the LLAMA3\\nmodel, offering configurations of 8 billion and 70 billion parameters. Thanks to extensive pre-training\\non more than 15 trillion data tokens, the LLAMA3 models3have achieved state-of-the-art (SOTA)\\nperformance across a broad range of tasks, establishing the LLaMA family as among the finest\\nopen-source LLMs available for a wide variety of applications and deployment scenarios.\\nDespite their impressive performance, deploying LLAMA3 models still poses significant challenges\\ndue to resource limitations in many scenarios. Fortunately, low-bit quantization has emerged as one\\nof the most popular techniques for compressing LLMs. This technique reduces the memory and\\ncomputational requirements of LLMs during inference, enabling them to run on resource-limited\\ndevices. Addressing the performance drop that occurs after compression is a major concern for\\ncurrent LLM quantization approaches. While numerous low-bit quantization methods have been\\nproposed, their evaluations have primarily focused on the earlier and less capable LLaMA models\\n(LLAMA1 andLLAMA2 ). Thus, LLAMA3 presents a new opportunity for the LLM community\\nto assess the performance of quantization on cutting-edge LLMs and to understand the strengths\\nand limitations of existing methods. In this empirical study, our aim is to analyze the capability of\\nLLAMA3 to handle the challenges associated with degradation due to quantization.\\nOur study sets out two primary technology tracks for quantizing LLMs: Post-Training Quantization\\n(PTQ) and LoRA-FineTuning (LoRA-FT) quantization, with the aim of providing a comprehensive\\nevaluation of the LLAMA3 models’ quantization. We explore a range of cutting-edge quantization\\nmethods across technical tracks (RTN, GPTQ [ 6], AWQ [ 10], SmoothQuant [ 20], PB-LLM [ 16],\\nQuIP [ 2], DB-LLM [ 3], and BiLLM [ 9] for PTQ; QLoRA [ 5] and IR-QLoRA [ 13] for LoRA-FT),\\ncovering a wide spectrum from 1 to 8 bits and utilizing a diverse array of evaluation datasets, including\\nWikiText2, C4, PTB, CommonSenseQA datasets (PIQA, ARC-e, ARC-c, HellaSwag, Winogrande),\\nand MMLU benchmark. The overview of our study is presented as Figure 1. These evaluations\\nassess the capabilities and limits of the LLAMA3 model under current LLM quantization techniques\\nand serve as a source of inspiration for the design of future LLM quantization methods. The choice\\nto focus specifically on the LLAMA3 model is motivated by its superior performance among all\\ncurrent open-source instruction-tuned LLMs across a variety of datasets3, including 5-shot MMLU,\\n0-shot GPQA, 0-shot HumanEval, 8-shot CoT GSM-8K, and 4-shot CoT MATH. Furthermore, we\\nhave made our project and the quantized models available to the public on https://github.com/\\nMacaronlin/LLaMA3-Quantization andhttps://huggingface.co/LLMQ , respectively. This\\nnot only aids in advancing the research within the LLM quantization community but also facilitates a\\nbroader understanding and application of effective quantization techniques.\\n2 Empirical Evaluation\\n2.1 Experiment Settings\\nEvaluated LLMs. We obtain the pre-trained LLAMA3 -8B and -70B through the official repository3.\\nQuantization methods. To evaluate the performance of low-bit quantized LLAMA3 , we select\\nrepresentative LLM quantization methods with extensive influence and functionality, including 8\\n3https://github.com/meta-llama/llama3\\n2', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d8f78229-adb1-42db-91d9-c60fec30fb0a', embedding=None, metadata={'page_label': '3', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Embeddings_Rag\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-08', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 1: Evaluation results of post-training quantization on LL AMA3-8B model\\nMethod #W #A #GPPL↓ CommonSenseQA ↑\\nWikiText2 C4 PTB PIQA ARC-e ARC-c HellaSwag Wino Avg.\\nLLAMA3 16 16 - 6.1 9.2 10.6 79.9 80.1 50.4 60.2 72.8 68.6\\nRTN4 16 128 8.5 13.4 14.5 76.6 70.1 45.0 56.8 71.0 63.9\\n3 16 128 27.9 1.1e2 95.6 62.3 32.1 22.5 29.1 54.7 40.2\\n2 16 128 1.9E3 2.5E4 1.8E4 53.1 24.8 22.1 26.9 53.1 36.0\\n8 16 - 6.2 9.5 11.2 79.7 80.8 50.4 60.1 73.4 68.9\\n4 16 - 8.7 14.0 14.9 75.0 68.2 39.4 56.0 69.0 61.5\\n3 16 - 2.2E3 5.6E2 2.0E3 56.2 31.1 20.0 27.5 53.1 35.6\\n2 16 - 2.7E6 7.4E6 3.1E6 53.1 24.7 21.9 25.6 51.1 35.3\\nGPTQ4 16 128 6.5 10.4 11.0 78.4 78.8 47.7 59.0 72.6 67.3\\n3 16 128 8.2 13.7 15.2 74.9 70.5 37.7 54.3 71.1 61.7\\n2 16 128 2.1E2 4.1E4 9.1E2 53.9 28.8 19.9 27.7 50.5 36.2\\n8 16 - 6.1 9.4 10.6 79.8 80.1 50.2 60.2 72.8 68.6\\n4 16 - 7.0 11.8 14.4 76.8 74.3 42.4 57.4 72.8 64.8\\n3 16 - 13.0 45.9 37.0 60.8 38.8 22.3 41.8 60.9 44.9\\n2 16 - 5.7E4 1.0E5 2.7E5 52.8 25.0 20.5 26.6 49.6 34.9\\nAWQ4 16 128 6.6 9.4 11.1 79.1 79.7 49.3 59.1 74.0 68.2\\n3 16 128 8.2 11.6 13.2 77.7 74.0 43.2 55.1 72.1 64.4\\n2 16 128 1.7E6 2.1E6 1.8E6 52.4 24.2 21.5 25.6 50.7 34.9\\n8 16 - 6.1 8.9 10.6 79.6 80.3 50.5 60.2 72.8 68.7\\n4 16 - 7.1 10.1 11.8 78.3 77.6 48.3 58.6 72.5 67.0\\n3 16 - 12.8 16.8 24.0 71.9 66.7 35.1 50.7 64.7 57.8\\n2 16 - 8.2E5 8.1E5 9.0E5 55.2 25.2 21.3 25.4 50.4 35.5\\nQuIP4 16 - 6.5 11.1 9.5 78.2 78.2 47.4 58.6 73.2 67.1\\n3 16 - 7.5 11.3 12.6 76.8 72.9 41.0 55.4 72.5 63.7\\n2 16 - 85.1 1.3E2 1.8E2 52.9 29.0 21.3 29.2 51.7 36.8\\nDB-LLM 2 16 128 13.6 19.2 23.8 68.9 59.1 28.2 42.1 60.4 51.8\\nPB-LLM2 16 128 24.7 79.2 65.6 57.0 37.8 17.2 29.8 52.5 38.8\\n1.7 16 128 41.8 2.6E2 1.2E2 52.5 31.7 17.5 27.7 50.4 36.0\\nBiLLM 1.1 16 128 28.3 2.9E2 94.7 56.1 36.0 17.7 28.9 51.0 37.9\\nSmoothQuant8 8 - 6.3 9.2 10.8 79.5 79.7 49.0 60.0 73.2 68.3\\n6 6 - 7.7 11.8 12.5 76.8 75.5 45.0 56.9 69.0 64.6\\n4 4 - 4.3E3 4.0E3 3.6E3 54.6 26.3 20.0 26.4 50.3 35.5\\nPTQ methods and 2 LoRA-FT methods. The implementations of our evaluated quantization methods\\nfollow their open-source repositories4. We also used eight NVIDIA A800 with 80GB GPU memory\\nfor quantitative evaluation.\\nEvaluation datasets. For the PTQ methods, we evaluate quantized LLAMA3 on the WikiText2 [ 12],\\nPTB [ 11], and a portion of the C4 dataset [ 14], using Perplexity (PPL) as the evaluation metric.\\nSubsequently, we further conduct experiments on five zero-shot evaluation tasks (PIQA [ 1], Wino-\\ngrande [ 15], ARC-e [ 4], ARC-c [ 4], and Hellaswag [ 22]) to fully validate the quantized performance\\nofLLAMA3 . For the LoRA-FT methods, we conduct the evaluation on the 5-shot MMLU bench-\\nmark [7] while also validating the aforementioned 5 zero-shot datasets for the LoRA-FT methods.\\nFor the fairness of our evaluation, we uniformly use WikiText2 as the calibration dataset for all\\nquantization methods, with a sample size of 128 and a consistent token sequence length of 2048.\\nFurthermore, for quantization methods requiring channel-wise grouping, we adopt a block size of\\n128 to balance performance and inference efficiency, which is a common practice in existing works.\\n4https://github.com/IST-DASLab/gptq ,https://github.com/mit-han-lab/llm-awq ,https:\\n//github.com/mit-han-lab/smoothquant ,https://github.com/Cornell-RelaxML/QuIP ,https:\\n//github.com/hahnyuan/PB-LLM ,https://github.com/Aaronhuang-778/BiLLM ,https://github.\\ncom/artidoro/qlora ,https://github.com/htqin/IR-QLoRA\\n3', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='d81ef002-1bd7-4246-a584-48f385758ede', embedding=None, metadata={'page_label': '4', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Embeddings_Rag\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-08', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 2: Evaluation results of post-training quantization on LL AMA3-70B model\\nMethod #W #A #GPPL↓ CommonSenseQA ↑\\nWikiText2 C4 PTB PIQA ARC-e ARC-c HellaSwag Wino Avg.\\nLLAMA3 16 16 - 2.9 6.9 8.2 82.4 86.9 60.3 66.4 80.6 75.3\\nRTN4 16 128 3.6 8.9 9.1 82.3 85.2 58.4 65.6 79.8 74.3\\n3 16 128 11.8 22.0 26.3 64.2 48.9 25.1 41.1 60.5 48.0\\n2 16 128 4.6E5 4.7E5 3.8E5 53.2 23.9 22.1 25.8 53.0 35.6\\nGPTQ4 16 128 3.3 6.9 8.3 82.9 86.3 58.4 66.1 80.7 74.9\\n3 16 128 5.2 10.5 9.7 80.6 79.6 52.1 63.5 77.1 70.6\\n2 16 128 11.9 22.8 31.6 62.7 38.9 24.6 41.0 59.9 45.4\\nAWQ4 16 128 3.3 7.0 8.3 82.7 86.3 59.0 65.7 80.9 74.9\\n3 16 128 4.8 8.0 9.0 81.4 84.7 58.0 63.5 78.6 73.2\\n2 16 128 1.7E6 1.4E6 1.5E6 52.2 25.5 23.1 25.6 52.3 35.7\\nQuIP4 16 - 3.4 7.1 8.4 82.5 86.0 58.7 65.7 79.7 74.5\\n3 16 - 4.7 8.0 8.9 82.3 83.3 54.9 63.9 78.4 72.5\\n2 16 - 13.0 22.2 24.9 65.3 48.9 26.5 40.9 61.7 48.7\\nPB-LLM2 16 128 11.6 34.5 27.2 65.2 40.6 25.1 42.7 56.4 46.0\\n1.7 16 128 18.6 65.2 55.9 56.5 49.9 25.8 34.9 53.1 44.1\\nBiLLM 1.1 16 128 17.1 77.7 54.2 58.2 46.4 25.1 37.5 53.6 44.2\\nSmoothQuant8 8 - 2.9 6.9 8.2 82.2 86.9 60.2 66.3 80.7 75.3\\n6 6 - 2.9 6.9 8.2 82.4 87.0 59.9 66.1 80.6 75.2\\n4 4 - 9.6 16.9 17.7 76.9 75.8 43.5 52.9 58.9 61.6\\n2.2 Track1: Post-Training Quantization\\nAs shown in Table 1 and Table 2, we provide the performance of low-bit LLAMA3 -8B and LLAMA3 -\\n70B with 8 different PTQ methods, respectively, covering a wide bit-width spectrum from 1 to 8-bit.\\nAmong them, Round-To-Nearest (RTN) is a vanilla rounding quantization method. GPTQ [ 6] is\\ncurrently one of the most efficient and effective weight-only quantization methods, which utilizes\\nerror compensation in quantization. But under 2-3 bits, GPTQ causes severe accuracy collapse when\\nquantized LLAMA3 . AWQ [ 10] adopts an anomaly channel suppression approach to reduce the\\ndifficulty of weight quantization, and QuIP [ 2] ensures the incoherence between weights and Hessian\\nby optimizing matrix computation. Both of them can keep LLAMA3 ’s capability at 3-bit and even\\npush the 2-bit quantization to promising.\\nThe recent emergence of binarized LLM quantization methods has realized ultra-low bit-width LLM\\nweight compression. PB-LLM [ 16] employs a mixed-precision quantization strategy, retaining a\\nsmall portion of significant weight full-precision while quantizing the majority of weights to 1-bit.\\nDB-LLM [ 3] achieves efficient LLM compression through double binarization weight splitting\\nand proposes a deviation-aware distillation strategy to further enhance 2-bit LLM performance.\\nBiLLM [ 9] further pushes the LLM quantization boundary to as low as 1.1-bit through residual\\napproximation of salient weights and grouped quantization of non-salient weights. These LLM\\nquantization methods specially designed for ultra-low bit-width can achieve higher accuracy of\\nquantized LLAMA3 -8B at⩽2-bit, far outperforms methods like GPTQ, AWQ, and QuIP under\\n2-bit (even 3-bit some cases).\\nWe also perform LLAMA3 evaluation on quantized activations via SmoothQuant [ 20], which moves\\nthe quantization difficulty offline from activations to weights to smooth out activation outliers. Our\\nevaluation shows that SmoothQuant can retain the accuracy of LLAMA3 with 8- and 6-bit weights\\nand activations, but faces collapse at 4-bit.\\nMoreover, we find that the LLAMA3 -70B model shows significant robustness for various quantization\\nmethods, even in ultra-low bit-width.\\n4', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='62bad4f6-592b-44a4-b25c-e1638ffc48be', embedding=None, metadata={'page_label': '5', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Embeddings_Rag\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-08', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3: LoRA-FT on LL AMA3-8B with Alpaca dataset\\nMethod #WMMLU ↑ CommonSenseQA ↑\\nHums. STEM Social Other Avg. PIQA ARC-e ARC-c HellaSwag Wino Avg.\\nLLAMA3 16 59.0 55.3 76.0 71.5 64.8 79.9 80.1 50.4 60.2 72.8 68.6\\nNormalFloat 4 56.8 52.9 73.6 69.4 62.5 78.6 78.5 46.2 58.8 74.3 67.3\\nQLoRA 4 50.3 49.3 65.8 64.2 56.7 76.6 74.8 45.0 59.4 67.0 64.5\\nIR-QLoRA 4 52.2 49.0 66.5 63.1 57.2 76.3 74.3 45.3 59.1 69.5 64.9\\n2.3 Track2: LoRA-FineTuning Quantization\\nExcept for the PTQ methods, we also provide the performance of 4-bit LLAMA3 -8B with 2 different\\nLoRA-FT quantization methods as shown in Table 3, including QLoRA [5] and IR-QLoRA [13].\\nOn the MMLU dataset, the most notable observation with LLAMA3 -8B under LoRA-FT quantization\\nis that low-rank finetuning on the Alpaca [ 17] dataset not only cannot compensate for the errors\\nintroduced by quantization, even making the degradation more severe. Specifically, various LoRA-FT\\nquantization methods obtain worse performance quantized LLAMA3 under 4-bit compared with their\\n4-bit counterparts without LoRA-FT. This is in stark contrast to similar phenomena on LLAMA1\\nandLLAMA2 , where, for the front one, the 4-bit low-rank finetuned quantized versions could\\neven easily surpass the original FP16 counterpart on MMLU. According to our intuitive analysis,\\nthe main reason for this phenomenon is due to LLAMA3 ’s strong performance brought by its\\nmassive pre-scale training, which means the performance loss from the original model’s quantization\\ncannot be compensated for by finetuning on a tiny set of data with low-rank parameters (which can\\nbe seen as a subset of the original model [ 8,5]). Despite the significant drop from quantization\\nthat cannot be compensated by finetuning, 4-bit LoRA-FT quantized LLAMA3 -8B significantly\\noutperforms LLAMA1 -7B and LLAMA2 -7B under various quantization methods. For instance, with\\nthe QLoRA method, 4-bit LLAMA3 -8B has an average accuracy of 57.0 (FP16: 64.8), exceeding\\n4-bit LLAMA1 -7B’s 38.4 (FP16: 34.6) by 18.6, and surpassing 4-bit LLAMA2 -7B’s 43.9 (FP16:\\n45.5) by 13.1 [ 21,13]. This implies that a new LoRA-FT quantization paradigm is needed in the era\\nof LL AMA3.\\nA similar phenomenon occurs with the CommonSenseQA benchmark. Compared to the 4-bit\\ncounterparts without LoRA-FT, the performance of the models fine-tuned using QLoRA and IR-\\nQLoRA also declined ( e.g.QLoRA 2.8% vs IR-QLoRA 2.4% on average). This further demonstrates\\nthe strength of using high-quality datasets in LLAMA3 , as the general dataset Alpaca does not\\ncontribute to the model’s performance in other tasks.\\n3 Conclusion\\nMeta’s recently released LLAMA3 models have rapidly become the most powerful LLM series, cap-\\nturing significant interest from researchers. Building on this momentum, our study aims to thoroughly\\nevaluate the performance of LLAMA3 across a variety of low-bit quantization techniques, including\\npost-training quantization and LoRA-finetuning quantization. Our goal is to assess the boundaries\\nof its capabilities in scenarios with limited resources by leveraging existing LLM quantization tech-\\nnologies. Our findings indicate that while LLAMA3 still demonstrates superior performance after\\nquantization, the performance degradation associated with quantization is significant and can even\\nlead to larger declines in many cases. This discovery highlights the potential challenges of deploying\\nLLAMA3 in resource-constrained environments and underscores the ample room for growth and\\nimprovement within the context of low-bit quantization. The empirical insights from our research are\\nexpected to be valuable for the development of future LLM quantization techniques, especially in\\nterms of narrowing the performance gap with the original models. By addressing the performance\\ndegradation caused by low-bit quantization, we anticipate that subsequent quantization paradigms\\nwill enable LLMs to achieve stronger capabilities at a lower computational cost, ultimately driving\\nthe progress of generative artificial intelligence, as represented by LLMs, to new heights.\\n5', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='76a2efe9-2073-4824-8dc6-46f118cfc7df', embedding=None, metadata={'page_label': '6', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Embeddings_Rag\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-08', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References\\n[1]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-\\nical commonsense in natural language. In Proceedings of the AAAI conference on artificial\\nintelligence , volume 34, pages 7432–7439, 2020.\\n[2]Jerry Chee, Yaohui Cai, V olodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantiza-\\ntion of large language models with guarantees. Advances in Neural Information Processing\\nSystems , 36, 2024.\\n[3]Hong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min\\nZhang, Jinyang Guo, Xianglong Liu, et al. Db-llm: Accurate dual-binarization for efficient llms.\\narXiv preprint arXiv:2402.11960 , 2024.\\n[4]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\\nchallenge. arXiv preprint arXiv:1803.05457 , 2018.\\n[5]Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\\nfinetuning of quantized llms. Advances in Neural Information Processing Systems , 36, 2024.\\n[6]Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\\n[7]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300 , 2020.\\n[8]Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\\nChen, et al. Lora: Low-rank adaptation of large language models. In International Conference\\non Learning Representations , 2021.\\n[9]Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele\\nMagno, and Xiaojuan Qi. Billm: Pushing the limit of post-training quantization for llms. arXiv\\npreprint arXiv:2402.04291 , 2024.\\n[10] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq:\\nActivation-aware weight quantization for llm compression and acceleration. arXiv preprint\\narXiv:2306.00978 , 2023.\\n[11] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark\\nFerguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate\\nargument structure. In Human Language Technology: Proceedings of a Workshop held at\\nPlainsboro, New Jersey, March 8-11, 1994 , 1994.\\n[12] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\\nmodels. arXiv preprint arXiv:1609.07843 , 2016.\\n[13] Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xi-\\nanglong Liu, and Michele Magno. Accurate lora-finetuning quantization of llms via information\\nretention. arXiv preprint arXiv:2402.05445 , 2024.\\n[14] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\\ntext-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.\\n[15] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\\nadversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,\\n2021.\\n[16] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm: Partially binarized large\\nlanguage models. arXiv preprint arXiv:2310.00034 , 2023.\\n6', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='dfd99b60-a42b-418e-ac11-663a3972809b', embedding=None, metadata={'page_label': '7', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Embeddings_Rag\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-08', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[17] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\\nhttps://github.com/tatsu-lab/stanford_alpaca , 2023.\\n[18] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems , 30, 2017.\\n[20] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.\\nSmoothquant: Accurate and efficient post-training quantization for large language models.\\nInInternational Conference on Machine Learning , pages 38087–38099. PMLR, 2023.\\n[21] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen,\\nXiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large\\nlanguage models. arXiv preprint arXiv:2309.14717 , 2023.\\n[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\\n7', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('./documents').load_data()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = embed_model\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM (Large Language Model) quantization refers to the process of reducing the bit width or precision of a large language model's weights and activations, while minimizing the loss of accuracy. This technique involves compressing the model to make it more computationally efficient and memory-friendly, without sacrificing its performance. Quantization can be achieved through various methods, including post-training quantization and LoRA-finetuning quantization. The goal is to enable LLMs to run at a lower computational cost, ultimately driving progress in generative artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# Run a query\n",
    "response = query_engine.query(\"what is llm quatization in 100 words?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "temp = str(response)\n",
    "temp = temp.split(\" \")\n",
    "\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "texts = [doc.text for doc in documents]\n",
    "\n",
    "# Get embeddings for all documents\n",
    "embeddings = embed_model.get_text_embedding_batch(texts)\n",
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Tell me the performance diffrence between non quantizd and quantized models?\n",
      "Answer: According to the text, the performance difference between non-quantized and quantized models is significant. Specifically:\n",
      "\n",
      "* On the WikiText2 dataset, the Perplexity (PPL) of the non-quantized LLAMA3 model is 6.1, while the PPL of some quantization methods (e.g., GPTQ4, AWQ4, QuIP4) ranges from 6.5 to 13.0, indicating a significant degradation in performance.\n",
      "* On the PTB dataset, the non-quantized LLAMA3 model has a Perplexity (PPL) of 9.2, while some quantization methods (e.g., GPTQ4, AWQ4) have PPL values ranging from 10.4 to 19.2.\n",
      "* On the zero-shot evaluation tasks (PIQA, Wino, ARC-e, ARC-c, and Hellaswag), the performance of the non-quantized LLAMA3 model is generally better than that of the quantization methods.\n",
      "\n",
      "However, it's worth noting that some quantization methods (e.g., SmoothQuant) have relatively small performance differences compared to the non-quantized model.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def query_documents(query_text, top_k=3):\n",
    "    # Get the embedding for the query\n",
    "    query_embedding = embed_model.get_text_embedding(query_text)\n",
    "    \n",
    "    # Compute cosine similarities\n",
    "    similarities = cosine_similarity([query_embedding], embeddings_array)[0]\n",
    "    \n",
    "    # Get indices of top-k most similar documents\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    # Return top-k documents and their similarities\n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'text': documents[idx].text,\n",
    "            'similarity': similarities[idx],\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_answer(query, context):\n",
    "    # Prepare the prompt for Llama\n",
    "    prompt = f\"\"\"Context information is below.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {query}\n",
    "Answer: \"\"\"\n",
    "\n",
    "    # Generate the answer using Llama\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "def rag_pipeline(query):\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = query_documents(query)\n",
    "    \n",
    "    # Prepare context by joining the text of relevant documents\n",
    "    context = \"\\n\\n\".join([doc['text'] for doc in relevant_docs])\n",
    "    \n",
    "    # Generate answer using Llama\n",
    "    answer = generate_answer(query, context)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "query = \"Tell me the performance diffrence between non quantizd and quantized models?\"\n",
    "answer = rag_pipeline(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
